"""Training module"""

import os
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint

class ModelTrainer:
    def __init__(self, model, model_name, save_dir='./saved_models'):
        self.model = model
        self.model_name = model_name
        self.save_dir = save_dir
        self.history = None
        os.makedirs(save_dir, exist_ok=True)
        
    def train(self, X_train, y_train, X_val, y_val, epochs=30, batch_size=32, learning_rate=0.001):
        """Train the model"""
        
        # Data augmentation
        aug = tf.keras.Sequential([
            layers.RandomRotation(0.1),
            layers.RandomTranslation(0.1, 0.1),
            layers.RandomZoom(0.1),
        ])
        
        # Compile
        self.model.compile(
            optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),
            loss='categorical_crossentropy',
            metrics=['accuracy', tf.keras.metrics.AUC(name='auc'),
                     tf.keras.metrics.Precision(name='precision'),
                     tf.keras.metrics.Recall(name='recall')]
        )
        
        # Callbacks
        callbacks = [
            EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1),
            ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1),
            ModelCheckpoint(
                os.path.join(self.save_dir, f'{self.model_name}_best.weights.h5'),
                monitor='val_accuracy', save_best_only=True, 
                save_weights_only=True, verbose=1
            )
        ]
        
        # Create datasets
        train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train))
        train_ds = train_ds.shuffle(1000).batch(batch_size).map(
            lambda x, y: (aug(x, training=True), y)
        ).prefetch(tf.data.AUTOTUNE)
        
        val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)) \
                                 .batch(batch_size).prefetch(tf.data.AUTOTUNE)
        
        # Train
        self.history = self.model.fit(
            train_ds, validation_data=val_ds,
            epochs=epochs, callbacks=callbacks, verbose=1
        )
        
        return self.history